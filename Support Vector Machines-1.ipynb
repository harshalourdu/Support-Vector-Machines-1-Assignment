{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?\n",
    "A Support Vector Machine (SVM) aims to find a hyperplane that best separates the data into two classes. For a linear SVM, the hyperplane can be represented by the equation:\n",
    "\n",
    "ùë§\n",
    "‚ãÖ\n",
    "ùë•\n",
    "+\n",
    "ùëè\n",
    "=\n",
    "0\n",
    "w‚ãÖx+b=0\n",
    "Where:\n",
    "\n",
    "ùë§\n",
    "w is the weight vector (perpendicular to the hyperplane),\n",
    "ùë•\n",
    "x is the input feature vector (a data point),\n",
    "ùëè\n",
    "b is the bias term (offset from the origin).\n",
    "The SVM aims to maximize the margin, which is the distance between the hyperplane and the closest data points from both classes (called support vectors).\n",
    "\n",
    "# Q2. What is the objective function of a linear SVM?\n",
    "The objective of a linear SVM is to maximize the margin while ensuring that all the data points are correctly classified (if possible). The optimization problem can be formulated as:\n",
    "\n",
    "Minimize the following objective function:\n",
    "\n",
    "This ensures that each point is on the correct side of the margin and at least 1 unit away from the hyperplane.\n",
    "\n",
    "# Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick allows SVM to operate in higher-dimensional spaces without explicitly computing the coordinates in those spaces. It maps the data from its original feature space into a higher-dimensional space where a linear separation is possible, even if the data is not linearly separable in the original space.\n",
    "\n",
    "The kernel function computes the dot product of the data points in the higher-dimensional space without explicitly transforming the data. Common kernel functions are:\n",
    "\n",
    "The kernel trick enables SVM to efficiently perform well in complex, non-linearly separable problems.\n",
    "\n",
    "# Q4. What is the role of support vectors in SVM? Explain with example\n",
    "Support vectors are the data points that are closest to the hyperplane and are critical in defining the optimal separating hyperplane. These points essentially \"support\" the hyperplane by lying on the margins, and their positions directly influence the optimal placement of the hyperplane.\n",
    "\n",
    "Example: Consider a simple 2D classification problem where two classes (positive and negative) are represented by circles and squares. The support vectors are the points that lie closest to the decision boundary (hyperplane), and they will determine the orientation and position of the hyperplane. If we move a support vector, the position of the hyperplane changes.\n",
    "\n",
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?\n",
    "Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the two classes. In a 2D space, it is simply a line, and in higher dimensions, it is a plane.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane is a boundary that is at an equal distance from the support vectors of both classes. It maximizes the margin between the classes.\n",
    "\n",
    "Hard Margin:\n",
    "A hard margin SVM is used when the data is perfectly linearly separable. In this case, there is no misclassification of any data points.\n",
    "\n",
    "Soft Margin:\n",
    "A soft margin SVM allows some misclassifications of data points to create a better generalization by allowing a slack variable \n",
    "ùúâ\n",
    "ùëñ\n",
    "Œæ \n",
    "i\n",
    "‚Äã\n",
    " . This is useful when the data is not perfectly linearly separable.\n",
    "\n",
    "Graphical Illustration:\n",
    "Hard Margin SVM: The margin is the largest distance between the two classes, and no points lie within the margin.\n",
    "Soft Margin SVM: The margin is still maximized, but some points may be inside the margin or misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. SVM Implementation through Iris dataset\n",
    "# Now, we‚Äôll implement an SVM classifier using the Iris dataset, and visualize how different values of the regularization parameter C affect the model‚Äôs performance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features (for 2D visualization)\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and testing set (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear SVM classifier with default C value (1.0)\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot decision boundary for the trained model\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create the contour plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=80)\n",
    "plt.title('SVM Decision Boundary (C=1.0)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# Try different values of C and see how it affects the model\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy with C={C}: {accuracy:.2f}')\n",
    "\n",
    "    # Plot decision boundary for different C values\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=80)\n",
    "    plt.title(f'SVM Decision Boundary (C={C})')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
